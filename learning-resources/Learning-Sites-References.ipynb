{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In mathematical optimization and decision theory, a loss function or cost function is a function that maps an event or values of one or more variables onto a real number intuitively representing some \"cost\" associated with the event. An optimization problem seeks to minimize a loss function. An objective function is either a loss function or its negative (in specific domains, variously called a reward function, a profit function, a utility function, a fitness function, etc.), in which case it is to be maximized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/hangtwenty/dive-into-machine-learning\n",
    "\n",
    "http://ncert.nic.in/textbook/textbook.htm\n",
    "\n",
    "https://www.iitbooks.co.in/2017/01/Concepts-of-Physics-HC-Verma-free-pdf.html\n",
    "\n",
    "https://www.kaggle.com/rahulparmarrp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://en.wikipedia.org/wiki/Loss_function\n",
    "\n",
    "https://en.wikipedia.org/wiki/Mean\n",
    "\n",
    "https://en.wikipedia.org/wiki/Mean_squared_error\n",
    "\n",
    "Just like Mean Squared Error/Deviation"
   ]
  },
  {
   "attachments": {
    "MSE.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![MSE.png](attachment:MSE.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The mean is halved as a convenience for the computation of the gradient descent, as the derivative term of the square function will cancel out the term."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://en.wikipedia.org/wiki/Gradient_descent\n",
    "\n",
    "https://ml-cheatsheet.readthedocs.io/en/latest/gradient_descent.html\n",
    "\n",
    "https://www.ritchieng.com/multi-variable-linear-regression/\n",
    "\n",
    "https://spin.atomicobject.com/2014/06/24/gradient-descent-linear-regression/\n",
    "\n",
    "http://people.exeter.ac.uk/dgbalken/BME05/\n",
    "\n",
    "https://archive.ics.uci.edu/ml/datasets/census+income\n",
    "\n",
    "https://livebook.manning.com/book/machine-learning-with-tensorflow/chapter-3/92\n",
    "\n",
    "https://www.geeksforgeeks.org/linear-regression-python-implementation/\n",
    "\n",
    "https://scikit-learn.org/stable/auto_examples/linear_model/plot_ols.html\n",
    "\n",
    "https://www.geeksforgeeks.org/univariate-linear-regression-in-python/\n",
    "\n",
    "https://www.geeksforgeeks.org/linear-regression-using-tensorflow/\n",
    "\n",
    "https://www.geeksforgeeks.org/ml-normal-equation-in-linear-regression/\n",
    "\n",
    "http://onlinestatbook.com/2/regression/intro.html\n",
    "\n",
    "https://newonlinecourses.science.psu.edu/stat462/node/101/\n",
    "\n",
    "https://data.world/nrippner/ols-regression-challenge # rahulparmarrp\n",
    "\n",
    "https://machinelearningmastery.com/implement-simple-linear-regression-scratch-python/\n",
    "\n",
    "https://www.math.muni.cz/~kolacek/docs/frvs/M7222/data/AutoInsurSweden.txt\n",
    "\n",
    "http://college.cengage.com/mathematics/brase/understandable_statistics/7e/students/datasets/slr/frames/slr06.html\n",
    "\n",
    "https://www.datacamp.com/courses/supervised-learning-with-scikit-learn\n",
    "\n",
    "https://seaborn.pydata.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\theta_j := \\theta_j - \\alpha \\frac{\\partial}{\\partial \\theta_j} J(\\theta_0, \\theta_1)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will know that we have succeeded when our cost function is at the very bottom of the pits in our graph, i.e. when its value is the minimum.\n",
    "\n",
    "The way we do this is by taking the derivative (the tangential line to a function) of our cost function. The slope of the tangent is the derivative at that point and it will give us a direction to move towards. We make steps down the cost function in the direction with the steepest descent, and the size of each step is determined by the parameter Î±, which is called the learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.holehouse.org/mlclass/\n",
    "\n",
    "http://cs229.stanford.edu/notes/\n",
    "\n",
    "https://www.ritchieng.com/machine-learning-resources/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://en.wikipedia.org/wiki/Linear_algebra\n",
    "\n",
    "https://www.deeplearningbook.org/\n",
    "\n",
    "https://www.math.ucdavis.edu/~linear/linear-guest.pdf\n",
    "\n",
    "http://vmls-book.stanford.edu/vmls.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple code for Cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost for  0.5  is  1.0833333333333333\n",
      "Cost for  1.0  is  0.08333333333333333\n",
      "Cost for  1.5  is  0.25\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# original data set\n",
    "X = [1, 2, 3]\n",
    "y = [1, 2.5, 3.5]\n",
    "\n",
    "# slope of best_fit_1 is 0.5\n",
    "# slope of best_fit_2 is 1.0\n",
    "# slope of best_fit_3 is 1.5\n",
    "\n",
    "hyps = [0.5, 1.0, 1.5] \n",
    "\n",
    "# mutiply the original X values by the theta \n",
    "# to produce hypothesis values for each X\n",
    "def multiply_matrix(mat, theta):\n",
    "    mutated = []\n",
    "    for i in range(len(mat)):\n",
    "        mutated.append(mat[i] * theta)\n",
    "\n",
    "    return mutated\n",
    "\n",
    "# calculate cost by looping each sample\n",
    "# subtract hyp(x) from y\n",
    "# square the result\n",
    "# sum them all together\n",
    "def calc_cost(m, X, y):\n",
    "    total = 0\n",
    "    for i in range(m):\n",
    "        squared_error = (y[i] - X[i]) ** 2\n",
    "        total += squared_error\n",
    "    \n",
    "    return total * (1 / (2*m))\n",
    "\n",
    "# calculate cost for each hypothesis\n",
    "for i in range(len(hyps)):\n",
    "    hyp_values = multiply_matrix(X, hyps[i])\n",
    "\n",
    "    print(\"Cost for \", hyps[i], \" is \", calc_cost(len(X), y, hyp_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0833333333333333\n",
      "0.08333333333333333\n",
      "0.25\n"
     ]
    }
   ],
   "source": [
    "#advance for cost function using linera algebra\n",
    "import numpy as np\n",
    "\n",
    "X = np.array([[1], [2], [3]])\n",
    "y = np.array([[1], [2.5], [3.5]])\n",
    "\n",
    "get_theta = lambda theta: np.array([[0, theta]])\n",
    "\n",
    "thetas = list(map(get_theta, [0.5, 1.0, 1.5]))\n",
    "\n",
    "X = np.hstack([np.ones([3, 1]), X])\n",
    "\n",
    "def cost(X, y, theta):\n",
    "    inner = np.power(((X @ theta.T) - y), 2)\n",
    "    return np.sum(inner) / (2 * len(X))\n",
    "\n",
    "for i in range(len(thetas)):\n",
    "    print(cost(X, y, thetas[i]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
